{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of a dataset using data mining techniques.\n",
    "\n",
    "This notebook focuses on exploring and mining basic information from an anonymised retail transactions dataset given by the company Instacart.\n",
    "\n",
    "## Taking a first glance at the dataset\n",
    "*Source code for this section may be find in file `dist/first-glance.class.ts`* \n",
    "\n",
    "The dataset consists of information about 3.4 million grocery orders, distributed across 6 `.csv` files listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'aisles.csv',\n",
      "  'departments.csv',\n",
      "  'formatted_itemsets.csv',\n",
      "  'order_id__product_number.csv',\n",
      "  'order_products__train.csv',\n",
      "  'orders.csv',\n",
      "  'product_id__order_number.csv',\n",
      "  'products.csv' ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "undefined"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    " * Folder in which the dataset files are.\n",
    " */\n",
    "const __foldername: string = 'instacart_basket_data';\n",
    "\n",
    "\n",
    "import * as cp from 'child_process';\n",
    "\n",
    "/**\n",
    " * Function executes a child_process listing the files in the instacart_basket_data folder\n",
    " * and returns the listed files though a Promise.\n",
    " */\n",
    "function listFiles(): Promise<string[]> {\n",
    "    // Async behavior\n",
    "    return new Promise( (resolve) => {\n",
    "        // Listing the files in the instacart_basket_data folder.\n",
    "        cp.exec(`ls ${__foldername}`)\n",
    "            .stdout.on('data', (data: string) => {\n",
    "                // Formatting ls output as an Array of strings (representing the file names)\n",
    "                resolve(data.match(/[^\\r\\n]+/g));\n",
    "            });\n",
    "    });\n",
    "}\n",
    "\n",
    "listFiles().then( (files: string[]) => console.log(files) );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files composing the dataset are listed below:\n",
    "\n",
    "```\n",
    "[ 'aisles.csv',\n",
    "  'departments.csv',\n",
    "  'order_products__prior.csv',\n",
    "  'order_products__train.csv',\n",
    "  'orders.csv',\n",
    "  'products.csv' ]\n",
    "```\n",
    "\n",
    "As a starting point, in order to have a first glance of the data we will actually be playing with throughout this entire report; let us display the first items composing each `.csv` file listed above. \n",
    "\n",
    "We will use NodeJS's straight-forward File I/O fs to list, open and `'csv-parse'`'s `Parser` to parse `.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "undefined"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import * as fs from 'fs';\n",
    "import{ Parser } from 'csv-parse';\n",
    "\n",
    "/**\n",
    " * Function reads a .csv file and returns it properlly formatted.\n",
    " */\n",
    "function readFile<T>(filePath: string): Promise<Array<T>> {\n",
    "    // Async behavior\n",
    "    return new Promise( (resolve, reject) => {\n",
    "        let ret: Array<T> = [];\n",
    "\n",
    "        // 'csv-parse' Parser, columns options groups each row by column in an object\n",
    "        let parser: Parser = new Parser({\n",
    "                delimiter: ',',\n",
    "                columns: true\n",
    "        });\n",
    "\n",
    "        parser\n",
    "            .on('data', (data: T) => ret.push(data) )\n",
    "            .on('error', (error: any) => reject(error) )\n",
    "            .on('end', () => resolve(ret) );\n",
    "\n",
    "        // Reading the file and piping to parser\n",
    "        fs.createReadStream(filePath).pipe(parser);\n",
    "    });\n",
    "}\n",
    "\n",
    "listFiles().then( (files: string[]) => {\n",
    "    files\n",
    "        .forEach( (file: string) => {\n",
    "            // Formatting to get absolute paths to files.\n",
    "            let filePath: string = `${__dirname}/${__foldername}/${file}`;\n",
    "\n",
    "            // Reading file\n",
    "            readFile<any>(filePath)\n",
    "                // Logging the first two elements of the array\n",
    "                .then( (data: Array<any>) => {\n",
    "                    console.log(`First two elements of file ${file}:`);\n",
    "                    console.log(data.slice(0,2));\n",
    "                })\n",
    "        })\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File `aisles.csv` is structured as such; and contains the two following rows:\n",
    "\n",
    "| aisle_id | aisle |\n",
    "|----------|-------|\n",
    "| 1 | prepared soups salads |\n",
    "| 1 | specialty cheeses |\n",
    "\n",
    "File `departments.csv` is structured as such; and contains the two following rows:\n",
    "\n",
    "| department_id | department |\n",
    "|---------------|------------|\n",
    "| 1 | frozen |\n",
    "| 2 | other |\n",
    "\n",
    "\n",
    "File `products.csv` is structured as such; and contains the two following rows:\n",
    "\n",
    "| product_id | product_name | aisle_id | department_id | \n",
    "|------------|--------------|----------|---------------|\n",
    "| 1 | Chocolate Sandwich Cookies | 61 | 19 |\n",
    "| 2 | All-Seasons Salt | 104 | 13 |\n",
    "\n",
    "\n",
    "Files `aisles.csv`, `departments.csv`, `products.csv` and all (trivially) contain information about the aisles, products, and departments names respectivelly; which may not be of any interet to us other than for enhanced visualisation. We'll thus have to perform the proper `JOIN`s (a.k.a. `UNION`) between these tables and our future data / pattern collections when needed.\n",
    "\n",
    "File `orders.csv` is surely a bit more interresting, especially having in mind **sequential pattern mining**, as it lists all the orders, and contains information on **when** and **by who** it has been placed.\n",
    "\n",
    "Finally, files `order_products__train.csv` and `order_products__prior.csv` contain the same, and most valuable information in regard to pattern mining, as they contain products ordered within each order.\n",
    "\n",
    "## Warming up : First statistics over the dataset\n",
    "*Source code for this section may be find in file `dist/stats.class.ts`* \n",
    "\n",
    "We'll keep our first analysis of the data simple, and start by computing some simple statistics over the dataset, on the number of orders,  products, products per order, etc. \n",
    "In addition to understanding more about the data, this will also allow us to find some appropriate and revelant criteria on which we could base (and reduce) our dataset for extended itemset analysis; as well as giving us a first glance at pattern dedundancy. \n",
    "\n",
    "Keeping in mind that each row of the `order_products__prior.csv` file has the following structure:\n",
    "- order_id\n",
    "- product_id\n",
    "- add_to_cart_order\n",
    "- reordered\n",
    "\n",
    "An interesting approach would be to regroup these objects by both `order_id` and `product_id`, as it would allow us to have a glance over the product distribution through the orders on the first hand, as well as idea of each product's popularity on the other.\n",
    "\n",
    "To do so, we need to transform the dataset consequently. Thus, let's start by defining `ProductOrder` as the structure of the data outputted by the `.csv` file parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(node:33876) UnhandledPromiseRejectionWarning: Unhandled promise rejection (rejection id: 1): Error: Number of columns on line 3 does not match header\n",
      "(node:33876) [DEP0018] DeprecationWarning: Unhandled promise rejections are deprecated. In the future, promise rejections that are not handled will terminate the Node.js process with a non-zero exit code.\n",
      "(node:33876) UnhandledPromiseRejectionWarning: Unhandled promise rejection (rejection id: 2): Error: Number of columns on line 14 does not match header\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two elements of file departments.csv:\n",
      "[ { department_id: '1', department: 'frozen' },\n",
      "  { department_id: '2', department: 'other' } ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "undefined"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    " * Data structure we gather from CSV File.\n",
    " */\n",
    "interface ProductOrder {\n",
    "    order_id: string,\n",
    "    product_id: string,\n",
    "    add_to_cart_order: string,\n",
    "    reordered: string\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, as we focus on exploiting data from file `order_products__prior.csv`, which contains more than 1 million records; and for the sake of memory usage, we'll try to work on data streams as much as possible, rather than parsing 1-million-elements-cached `Arrays` when it comes to data transformation.\n",
    "We'll thus be using Reactive Programming library `RxJS` in that intent.\n",
    "\n",
    "Reactive programming is nothing new as it only consists in programming with asynchronous data streams, which languages like JS are basically all about. `RxJS` yet provides us with an amazing and complete approach -as well as a great toolbox of functions- to combine, create and filter such streams easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by orders\n",
    "*Sources are available in file `dist/stats-on-orders.spaghetti.ts`.*\n",
    "\n",
    "Let us define a function allowing us to group `ProductOrder` objects by any key of the `ProductOrder` interface. We'll use in that intent `RxJS`'s `groupBy` method, which basically groups the items emitted by an `Observable` (a.k.a. stream; in our case, the `ReadStream` of the considered file) according to a specified criterion (in our case, either the `product_id` or the `order_id`), and emits these grouped items as `GroupedObservable`s, one `GroupedObservable` per group.\n",
    "\n",
    "Let's start by defining a `Group<T>` as an object containg an `id` (the grouping criterion basically), as well as an `Array` of whatever item of type `T` we're grouping. This will be one of many product of our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two elements of file aisles.csv:\n",
      "[ { aisle_id: '1', aisle: 'prepared soups salads' },\n",
      "  { aisle_id: '2', aisle: 'specialty cheeses' } ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "undefined"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interface Group<T> {\n",
    "    id: string,\n",
    "    items: Array<T>\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also forge ahead (keeping pattern mining in mind) by allowing this method to `.map()` the `ProductOrder` objects to whatever we want (its `id`, `product_name`...) depending on our need.\n",
    "\n",
    "Such a function is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import { Observable } from 'rxjs/Observable';\n",
    "import 'rxjs/add/operator/finally';\n",
    "import 'rxjs/add/operator/groupBy';\n",
    "import * as RxNode from 'rx-node';\n",
    "\n",
    "/**\n",
    " * Function returns an Observable of `ProductOrder` group by a defined criterion. You may map the parsed `ProductOrder` to whatever value you want.\n",
    " */\n",
    "function _readAndGroupBy<T>( key: keyof ProductOrder, map: (val: ProductOrder) => T ): Rx.Observable<Group<T>> {\n",
    "    /**\n",
    "     * 'csv-parse' Parser, columns options groups each row by column in an object.\n",
    "     */\n",
    "    let parser: Parser = new Parser({\n",
    "        delimiter: ',',\n",
    "        columns: true\n",
    "    });\n",
    "\n",
    "    // Turning native stream into Observable\n",
    "    return RxNode.fromStream( fs.createReadStream(`${__foldername}/order_products__train.csv`).pipe(parser) )\n",
    "        // Grouping objects by order\n",
    "        .groupBy( (data: ProductOrder) => data[key] )\n",
    "        // At this point, we basically have an Observable by group. Thus we need to flatten that.\n",
    "        .flatMap( (group: Rx.GroupedObservable<string, ProductOrder>) => {\n",
    "            return group\n",
    "                // Formatting the data\n",
    "                .map(map)\n",
    "                // And flattening the Observable array.\n",
    "                .reduce( (concat: Group<T>, current: T) => {\n",
    "                    concat.items.push(current);\n",
    "                    return concat;\n",
    "                }, {\n",
    "                    id: group.key,\n",
    "                    items: []\n",
    "                })\n",
    "        });\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us group the `ProductOrder` by their `order_id`. `ProductOrders` will be caracterized by their `product_id` (We'll thus trivially have an list of Orders (or itemsets), as `Arrays` of `product_id`s). \n",
    "\n",
    "Function above will return us with all the processed groups. We'll compute some basic statistics on these from there, such as: \n",
    "- The number of orders (number of groups);\n",
    "- The minimum number of product in an order (minimum of arrays length);\n",
    "- The maximum number of product in an order (maximum of arrays length);\n",
    "- The average product number per order (average array length);\n",
    "- The number of records in the `order_products__prior.csv` file (sum of arrays length);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering data, this might take a while...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "undefined"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function statsOnOrders(): void {\n",
    "    console.log('Gathering data, this might take a while...');\n",
    "    \n",
    "    /**\n",
    "     * All the groups.\n",
    "     */\n",
    "    let groups: Array<Group<string>> = [];\n",
    "\n",
    "    let stats: any = {\n",
    "        max: 0,\n",
    "        min: Infinity,\n",
    "        sum: 0\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * Reads the file and groups `ProductOrders as intended`\n",
    "     */\n",
    "\n",
    "    _readAndGroupBy<string>('order_id', (productOrder: ProductOrder) => productOrder.product_id )\n",
    "        // Once all groups are loaded, displaying them.\n",
    "        .finally( () => {\n",
    "            console.log(`Maximum number of ProductOrders: ${stats.max}`);\n",
    "            console.log(`Minimum number of ProductOrders: ${stats.min}`);\n",
    "            console.log(`Average number of ProductOrders: ${stats.sum / groups.length}`);\n",
    "            console.log(`Total number of ProductOrders: ${stats.sum}`);\n",
    "            console.log(`Number of itemsets: ${groups.length}`);\n",
    "        })\n",
    "        // Note that this behaviour (induced by the flatMap of readAndGroupBy) makes everything pretty much blocking again.\n",
    "        .subscribe( (group: Group<string>) => {\n",
    "            // Computing some basic stats on the fly\n",
    "            stats.max = Math.max(group.items.length, stats.max);\n",
    "            stats.min = Math.min(group.items.length, stats.min);\n",
    "            stats.sum += group.items.length;\n",
    "\n",
    "            // Pushing group to groups.\n",
    "            groups.push(group)\n",
    "        });\n",
    "}\n",
    "\n",
    "statsOnOrders();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping the records by their `order_id` enlights us of the following information:\n",
    "\n",
    "| Number of orders | Minimum product number per order | Maximum product number per order | Average product number per order | Total number of records |\n",
    "|-|-|-|-|-|-|\n",
    "| 131,209 | 1 | 80 | 10.6 | 1,384,617 |\n",
    "\n",
    "Some trivial modifications of the function above could allow us to retrieve the number of product per order for enhanced visualisation (code can be find in source files): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by product\n",
    "\n",
    "Creating itemsets (`Group`s) of Orders, based on the `product_id` of `ProductOrders` may also be of interest to us, as it allow us \"feel\" a product \"popularity\" by counting the number of orders it appears in. This is a pretty good deal in regards to pattern mining, as:\n",
    "- a \"frequent\" product will be more likely to appear in frequent itemsets;\n",
    "- its number of appearance in the collection is, by definition, the maximum support over the dataset. Considering a Product A being the most popular in a dataset such as ours, the itemset { A } will trivially be the absolute, most frequent itemset to be find in the entire dataset;\n",
    "- if a product is too frequent, it may be of interest to ignore it, as the itemsets to be find may not be revealant enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is basically the same as before, thus won't be included in the notebook. Sources are however still available in file `dist/stats-on-products.spaghetti.ts`.\n",
    "Grouping the records by their `product_id` gives us the following results:\n",
    "\n",
    "\n",
    "| Number of products | Minimum order number per product | Maximum order number per product | Average order number per product | Total number of records |\n",
    "|-|-|-|-|-|-|\n",
    "| 391,23 | 1 | 18,726 | 35.39 | 1,384,617 |\n",
    "\n",
    "Joining retrieved data with table `products.csv` using the (dirty, yet it works.) following function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "undefined"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/**\n",
    " * Function finds element of array of which the key corresponds to value; and returns another defined value of this element.\n",
    " */\n",
    "function join<T>( array: T[], initKey: keyof T, value: any, returnKey: keyof T): any {\n",
    "    let element: T = array.find((element: T) => element[initKey] == value );\n",
    "    return element[returnKey];\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... we are able to compare products popularity, and conclude on: \n",
    "- Banana being the most popular product, being ordered 18,726 times;\n",
    "- 46 products including 100% Black Cherry & Concord Grape Juice, Breaded Popcorn Turkey Dogs or Lip Balm, are the least popular with only 1 order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent item sets\n",
    "\n",
    "Knowing a little bit more about our data, we'll now move to mine and gather frequent itemsets from our **training** dataset (`order_products_train.csv`); in other words, **TODO**\n",
    "\n",
    "### Dataset formatting\n",
    "\n",
    "Upon this point we will be using SPMF library's JAVA implementation of Apriori (through command lines), in order to mine frequent item sets from our dataset. This implementation needs the data to be formatted as such: \n",
    "\n",
    "```\n",
    "A B C\n",
    "D E\n",
    "```\n",
    "\n",
    "With `{ A, B, C }` and `{ D, E }` representing itemsets, with `A, B, C, D, E` being integers exclusively. \n",
    "- Itemsets needs to separated by a `return carriage` character (`\\r\\n`);\n",
    "- Within itemsets, items' ids are separated by a plain `space` character.\n",
    "\n",
    "Transforming this dataset into this format is pretty straightforward using the code we already wrote previously; though for the sake of lisibility now we're done tinkering with the data, these functions have been clustered in a proper class `CSVParser`.\n",
    "\n",
    "The following code process the dataset into the proper format, and pushes it into a new `formatted_dataset.csv` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Cannot find module './class/csv-parser.class'",
     "output_type": "error",
     "traceback": [
      "module.js:529",
      "    throw err;",
      "    ^",
      "",
      "Error: Cannot find module './class/csv-parser.class'",
      "    at Function.Module._resolveFilename (module.js:527:15)",
      "    at Function.Module._load (module.js:476:23)",
      "    at Module.require (module.js:568:17)",
      "    at require (internal/module.js:11:18)",
      "    at evalmachine.<anonymous>:1:26",
      "    at ContextifyScript.Script.runInThisContext (vm.js:50:33)",
      "    at Object.runInThisContext (vm.js:139:38)",
      "    at run ([eval]:821:15)",
      "    at onRunRequest ([eval]:666:18)",
      "    at onMessage ([eval]:634:13)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First two elements of file order_id__product_number.csv:\n",
      "[ { '1': '36', '8': '8' }, { '1': '38', '8': '9' } ]\n",
      "First two elements of file products.csv:\n",
      "[ { product_id: '1',\n",
      "    product_name: 'Chocolate Sandwich Cookies',\n",
      "    aisle_id: '61',\n",
      "    department_id: '19' },\n",
      "  { product_id: '2',\n",
      "    product_name: 'All-Seasons Salt',\n",
      "    aisle_id: '104',\n",
      "    department_id: '13' } ]\n",
      "First two elements of file order_products__train.csv:\n",
      "[ { order_id: '1',\n",
      "    product_id: '49302',\n",
      "    add_to_cart_order: '1',\n",
      "    reordered: '1' },\n",
      "  { order_id: '1',\n",
      "    product_id: '11109',\n",
      "    add_to_cart_order: '2',\n",
      "    reordered: '1' } ]\n",
      "Maximum number of ProductOrders: 80\n",
      "Minimum number of ProductOrders: 1\n",
      "Average number of ProductOrders: 10.552759338155157\n",
      "Total number of ProductOrders: 1384617\n",
      "Number of itemsets: 131209\n",
      "First two elements of file orders.csv:\n",
      "[ { order_id: '2539329',\n",
      "    user_id: '1',\n",
      "    eval_set: 'prior',\n",
      "    order_number: '1',\n",
      "    order_dow: '2',\n",
      "    order_hour_of_day: '08',\n",
      "    days_since_prior_order: '' },\n",
      "  { order_id: '2398795',\n",
      "    user_id: '1',\n",
      "    eval_set: 'prior',\n",
      "    order_number: '2',\n",
      "    order_dow: '3',\n",
      "    order_hour_of_day: '07',\n",
      "    days_since_prior_order: '15.0' } ]\n"
     ]
    }
   ],
   "source": [
    "import { CSVParser } from './class/csv-parser.class';\n",
    "import { Group } from './interface/group.interface';\n",
    "import { Product } from './interface/product.interface';\n",
    "import { ProductOrder } from './interface/product-order.interface';\n",
    "\n",
    "export class FormatData {\n",
    "    private readonly __foldername: string = 'instacart_basket_data';\n",
    "    private _output: string[] = [];\n",
    "\n",
    "    constructor() {\n",
    "        new CSVParser<Product>(`${__dirname}/../${this.__foldername}/products.csv`).loadAll()\n",
    "            .then( (products: Product[]) => {\n",
    "                console.log('Products has been loaded');\n",
    "\n",
    "                new CSVParser<ProductOrder>(`${__dirname}/../${this.__foldername}/order_products__train.csv`)\n",
    "                    // Grouping items by order_id, and mapping every item composing these itemsets to their product_id.\n",
    "                    .generateItemsets<string>('order_id', (productOrder: ProductOrder) => productOrder.product_id)\n",
    "                    // Once execution is complete, writing the formatted dataset into a proper file.\n",
    "                    .finally( () => {\n",
    "                        // Writing number of product per order_id in a new file : The array of already formatted rows is joined by a return carriage character.\n",
    "                        fs.writeFile(`/Users/alexisfacques/Projects/python-apriori/formatted_itemsets.csv`, this._output.join('\\r\\n'), (err: any) => {\n",
    "                            if(err) return console.log(err);\n",
    "                            console.log('The file was saved!');\n",
    "                        });\n",
    "                    })\n",
    "                    // On group reception, formatting the items composing is as a ROW (joined by plain space character), and pushing it the output array.\n",
    "                    .subscribe((group: Group<ProductOrder,string>) => this._output.push(group.items.join(' ')) )\n",
    "            })\n",
    "    }\n",
    "}\n",
    "\n",
    "new FormatData();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One in the SPMF library: http://www.philippe-fournier-viger.com/spmf/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Typescript 2.5",
   "language": "typescript",
   "name": "typescript"
  },
  "language_info": {
   "file_extension": ".ts",
   "mimetype": "text/x-typescript",
   "name": "typescript",
   "version": "2.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
